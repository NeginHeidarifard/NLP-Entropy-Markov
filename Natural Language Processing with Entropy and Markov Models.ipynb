{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9155b0b",
   "metadata": {},
   "source": [
    "# Compression, Prediction, Generation: Text Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7becdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this project we are interested in compressing and generating texts written in natural languages.\n",
    "Given a text of length $n$, a sequence of symbols is just a vector $(x_1, . . . , x_n)$ where each $x_i$ is a symbol i.e. $x_i = a, b, c, \\dots$. We can define the alphabet of possible symbols as $\\mathcal{A} = \\{a_1,a_2,\\dots,a_M\\}$ then each $x_i$ can have $M$ values.\n",
    "\n",
    "In order to model the sequence of symbols we need a joint probability distribution for each symbol in the sequence, namely $p(X_1 = x_1, X_2 = x_2, \\dots , X_n = x_n)$. If our alphabet had $M$ symbols, for modelling a sequence of length $n$ we would need $M^n$ probabilities. Thus some assumptions are required in order to reduce this dimensionality. In this case we will use two different models for $p$, the IID and the Markov Chain model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27e3d5",
   "metadata": {
    "id": "AXFy0UMH_jDH"
   },
   "source": [
    "### IID Model\n",
    "The IID model assumes:\n",
    "\n",
    "$$ p(X_1 = x_1, X_2 = x_2, \\dots , X_n = x_n) = \\prod_{i=1}^n p(X_i = x_i)$$\n",
    "\n",
    "i.e. that the symbols in a sequence are independent and identically distributed. With this model we need only $M$ probabilities, one for each symbol. One can generalize and use symbols not of a single character but of multiples ones. For example using 3 characters per symbol, the symbols would be of the form $aaa,aab,...,zzz$. When using $k$ characters per symbols in an alphabet of $M$ characters, the needed probabilities would be $M^k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ff068",
   "metadata": {
    "id": "ZsX7DGxl5pF7"
   },
   "source": [
    "### Markov Chain Model\n",
    "\n",
    "The Markov Chain model assume a limited range of dependence of the symbols. Indeed for an order $k$ Markov Chain:\n",
    "\n",
    "\n",
    "$$p(X_i | X_{i-1},X_{i-2},\\dots,X_1) = p(X_i | X_{i-1},X_{i-2},\\dots,X_{i-k})$$\n",
    "\n",
    "\n",
    "The meaning of the above structure is that the $i$-th symbol in the sequence depends only on the previous $k$ symbols. We add the time *invariant assumption*, meaning that the conditional probabilities do not depend on the time index $i$ i.e. $p(X_i | X_{i-1},X_{i-2},\\dots,X_{i-k}) = p(X_{k+1} | X_{k},X_{k-1},\\dots,X_{1})$. The most common and widely used Markov Chain is the Markov Chain of order 1:\n",
    "\n",
    "$$p(X_i | X_{i-1},X_{i-2},\\dots,X_1) = p(X_i | X_{i-1})$$\n",
    "\n",
    "In this case the conditional probability $p(X_i|X_{iâˆ’1})$ can be expressed using $M^2$\n",
    "numbers. Usually this is referred to as the *transition matrix*. Given an alphabet $\\mathcal{A} = \\{a_1,a_2,\\dots,a_M\\}$ the transition matrix can be written as: \n",
    "\n",
    "$$ \\mathbb{M}_{kl} = p(X_i = a_k| X_{i-1} = a_l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de07c15",
   "metadata": {},
   "source": [
    "### Entropy and Cross-Entropy\n",
    "\n",
    "\n",
    "- For the IID model of order 1 the entropy computation is straightforward: \n",
    "$$ H_{IID} = -\\sum_{i=1}^M p(a_i) log p(a_i)$$ \n",
    "and consequently, starting from two distributions $p,q$ fitted on two different texts, the cross-entropy:\n",
    "$$ CE_{IID} = -\\sum_{i=1}^M p(a_i) log q(a_i)$$\n",
    "\n",
    "\n",
    "- For the MC model of order 1 the entropy is defined as follows: \n",
    "$$ H_{MC} = - \\sum_{kl} \\pi(a_k) p(X_i = a_k| X_{i-1} = a_l) log \\left(p(X_i = a_k| X_{i-1} = a_l)\\right)= - \\sum_{kl} \\pi_k\\mathbb{M}_{kl} log \\mathbb{M}_{kl}$$\n",
    "where $\\pi$ is the stationary distribution of the Markov Chain i.e. $\\pi_k = \\mathbb{M}_{kl} \\pi_l$. The code to compute the stationary distribution is already given.\n",
    "The cross-entropy:\n",
    "$$ CE_{IID} = - \\sum_{kl} \\pi_k\\mathbb{M}_{kl} log \\mathbb{M'}_{kl}$$\n",
    "with $\\mathbb{M}$ and $\\mathbb{M'}$ are fitted on two different texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6a81f",
   "metadata": {},
   "source": [
    "### Theoretical Questions: \n",
    "\n",
    "1) Interpret the time invariant assumption associated to our Markov chains in the contex of text generation.\n",
    "\n",
    "This assumtion is what simplifies the Markov model, byt making the probability of transitioning from one state to another independent from when it happens, and only dependent on the state(s) itself. This allows for the use of one transition matrix regardless of the position in the text making the Markov Model much simpler. Including position in the text would exponentially raise the number of parameters and it is a hard problem to model. \n",
    "\n",
    "2) How can we rewrite a Markov chain of higher order as a Markov chain of order 1?\n",
    "\n",
    "Easy, if we're dealing with an order n markov chain model, to make it order 1, we change our state space into tuples of n consecutive states in the original space. Meaning all our possible states will be all possible combinations of n states. Moving from one state to another would happen by dropping the first (oldest) state in the tuple, and adding on the new one. This would create the new tuple. \n",
    "\n",
    "3) Given a probability distribution over symbols, how to use it for generating sentences?\n",
    "\n",
    "We start by randomly sampling a symbol, or maybe it is given to us. \n",
    "\n",
    "We then use the proba dist to randomly sample the next symbol taking into account any conditions such as the dependancies on the previous states of the Markov Model. \n",
    "\n",
    "Append it to our text. \n",
    "\n",
    "Continue until we reach the maxlength or a stopping symbol is generated (stopping conditions differ). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eeceff",
   "metadata": {},
   "source": [
    "### Practical questions\n",
    "\n",
    "In order to construct our IID and Markov Chain models we need some text. Our source will be a set of classical novels available at: https://www.lri.fr/~gcharpia/informationtheory/TP2_texts.zip\n",
    "\n",
    "We will use the symbols in each text to learn the probabilities of each model. The alphabet we suggest for the characters to use is string.printable which is made of $\\sim 100$ characters. (see below)\n",
    "\n",
    "For both models, perform the following steps:\n",
    "\n",
    "1) For different orders of dependencies, train the model on a novel and compute the associated entropy. What do you observe as the order increases? Explain your observations.\n",
    "\n",
    "2) Use the other novels as test sets and compute the cross-entropy for each model trained previously. How to handle symbols (or sequences of symbols) not seen in the training set?\n",
    "\n",
    "3) For each order of dependencies, compare the cross-entropy with the entropy. Explain and interpret the differences.\n",
    "\n",
    "4) Choose the order of dependencies with the lowest cross-entropy and generate some sentences.\n",
    "\n",
    "5) Train one model per novel and use the KL divergence in order to cluster the novels.\n",
    "\n",
    "\n",
    "<b>Hints</b> : \n",
    "\n",
    "- In the MC case limit yourself to order $2$ (the computation can become quite expensive). If you have $ M \\sim 100$ characters, for order $1$ you will need a $\\sim 100 \\times 100$ matrix, for order $2$ a $\\sim 10^4 \\times 10^4$ matrix.\n",
    "\n",
    "- For the second order MC model you need to compute: $p(X_{i+1},X_{i}|X_{i},X_{i-1})$\n",
    "\n",
    "- It is possible to implement efficiently the two models with dictionaries inPython.  For the IID model, a key of the dictionary is simply a symbol and the value is the number of occurrences of the symbol in the text. For a Markov chain, a key of the dictionary is also a symbol, but the value is a vector that contains the number of occurrences of each character of the alphabet.  Notice that a symbol may consist of one or several characters. Note also that there is no need to explicitly consider all possible symbols; the ones that are observed in the training set are sufficient.\n",
    "\n",
    "- A low probability can be assigned to symbols not observed in the training-set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed41454",
   "metadata": {},
   "source": [
    "#### Computing stationary distribution \n",
    "\n",
    "Here we provide you two version of the function to compute the stationary distirbution of a markov chain and show a small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536b40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if(\"TP2_texts.zip\" not in os.listdir(\".\")):\n",
    "    !wget https://www.lri.fr/~gcharpia/informationtheory/TP2_texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363b0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#direct way to find pi (can be slow)\n",
    "import  numpy  as np\n",
    "\n",
    "def Compute_stationary_distribution(P_kl):\n",
    "    ## P_kl must be the transition matrix from state l to state k!\n",
    "    evals , evecs = np.linalg.eig(P_kl)   \n",
    "    evec1 = evecs[:,np.isclose(evals , 1)]\n",
    "    evec1 = evec1 [:,0]\n",
    "    pi = evec1 / evec1.sum()\n",
    "    pi = pi.real #stationary  probability\n",
    "    \n",
    "    return pi \n",
    "\n",
    "#iteative way (should be faster)\n",
    "def Compute_stationary_distribution_it(P_kl, n_it):\n",
    "    pi = np.random.uniform(size=P_kl.shape[0]) #initial state, can be a random one!\n",
    "    pi /= pi.sum()\n",
    "    #print(pi,pi.sum())\n",
    "    for t in range(n_it):   \n",
    "        pi = np.matmul(P_kl,pi)\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d57e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.625, 0.375])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##simple example of computation of stationary distribution \n",
    "\n",
    "n_it = 1000                                     ##remind to check that n_it is enough to reach convergence\n",
    "P_kl = np.array([[0.7,0.5],[0.3,0.5]])\n",
    "Compute_stationary_distribution_it(P_kl,n_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d775cd1",
   "metadata": {},
   "source": [
    "#### Defining the Alphabet\n",
    "\n",
    "Example of uploading a text and filtering out characters which are not in the chosen alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad8b6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  string\n",
    "\n",
    "def import_text(file_name):\n",
    "    lines = []\n",
    "    with  open(file_name , encoding='UTF8') as f:\n",
    "        lines = f.readlines ()\n",
    "        text = '\\n'.join(lines)\n",
    "        printable = set(string.printable)\n",
    "        text = ''.join(filter(lambda x: x in printable , text))     \n",
    "    return text\n",
    "\n",
    "text = import_text('./texts/Alighieri.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f69db4",
   "metadata": {},
   "source": [
    "#### IID - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1551fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter \n",
    "\n",
    "class IIDModel:\n",
    "    def __init__(self, order=1):\n",
    "        self.order = order\n",
    "        self.probabilities = {}  # Dictionary to store symbol probabilities\n",
    "    \n",
    "    def process(self, text):\n",
    "        \"\"\"Process the text to calculate symbol probabilities.\"\"\"\n",
    "        if self.order == 1:\n",
    "            total_symbols = len(text)\n",
    "            symbol_counts = Counter(text)  # Counts each symbol's occurrences in text\n",
    "            # probability of each symbol\n",
    "            self.probabilities = {symbol: count / total_symbols for symbol, count in symbol_counts.items()}\n",
    "        else:\n",
    "            # For higher order, group symbols\n",
    "            grouped_symbols = [text[i:i+self.order] for i in range(0, len(text) - self.order + 1)]\n",
    "            total_groups = len(grouped_symbols)\n",
    "            symbol_counts = Counter(grouped_symbols)\n",
    "            self.probabilities = {symbol: count / total_groups for symbol, count in symbol_counts.items()}\n",
    "    \n",
    "    def getEntropy(self):\n",
    "        \"\"\"Calculate and return the entropy of the model.\"\"\"\n",
    "        entropy = -sum(p * math.log(p, 2) for p in self.probabilities.values())\n",
    "        return entropy\n",
    "    \n",
    "    def getCrossEntropy(self, text):\n",
    "        \"\"\"Calculate the cross-entropy between the model and a given text, handling unseen symbols.\"\"\"\n",
    "        if self.order == 1:\n",
    "            symbols = text\n",
    "        else:\n",
    "            symbols = [text[i:i+self.order] for i in range(len(text) - self.order + 1)]\n",
    "        \n",
    "        unseen_prob = 1e-6  # Small probability for unseen symbols as suggested in tricks\n",
    "        total_symbols = len(symbols)\n",
    "        \n",
    "        cross_entropy = -sum(math.log(self.probabilities.get(symbol, unseen_prob), 2) for symbol in symbols) / total_symbols\n",
    "        return cross_entropy\n",
    "    \n",
    "    def generate(self, length):\n",
    "        \"\"\"Generate a text of a given length based on the model.\"\"\"\n",
    "        symbols, probabilities = zip(*self.probabilities.items())\n",
    "        generated_text = ''.join(np.random.choice(symbols, p=probabilities) for _ in range(length))\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1430d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def KL_divergence(dist1, dist2, epsilon=1e-06):\n",
    "    kl_div = 0.0\n",
    "    for event in dist1:\n",
    "        p = dist1[event]\n",
    "        q = dist2.get(event, epsilon)\n",
    "        \n",
    "        if p > 0 and q > 0:\n",
    "            kl_div += p * math.log(p / q)\n",
    "            \n",
    "    return kl_div\n",
    "\n",
    "def load_text(filepath):\n",
    "    \"\"\"Utility function to load text from a file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71abf9",
   "metadata": {},
   "source": [
    "## 1.\n",
    "As the order increases, we notice an increase in the entropy meaning an increase in uncertainty which at first sounds controversial. But if we think about it, increasing the order means exponentially increasing the number of possible states, while assuming an IID distribution. This has 2 main problems: \n",
    "\n",
    "1. A system with more states is automatically more uncertain.\n",
    "2. With a high number of states, without a huge corpus to clearly determine preferences, the model tends towards uniformity making entropy at its highest possible value. \n",
    "\n",
    "This clearly shows the mediocrity of the assumtions that the IID model makes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2efda6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order: 1, Entropy: 4.5090\n",
      "Order: 2, Entropy: 8.0591\n",
      "Order: 3, Entropy: 10.8014\n"
     ]
    }
   ],
   "source": [
    "novel_text = load_text('./texts/Dostoevsky.txt')\n",
    "test_novels = [\n",
    "    load_text('./texts/Alighieri.txt'),\n",
    "    load_text('./texts/Goethe.txt'),\n",
    "    load_text('./texts/Hamlet.txt')]\n",
    "\n",
    "orders = [1, 2, 3]  # Orders to test\n",
    "entropies = []\n",
    "\n",
    "for order in orders:\n",
    "    model = IIDModel(order=order)\n",
    "    model.process(novel_text)\n",
    "    entropy = model.getEntropy()\n",
    "    entropies.append((order, entropy))\n",
    "    print(f\"Order: {order}, Entropy: {entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e40dbc",
   "metadata": {},
   "source": [
    "## 2&3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cdd952",
   "metadata": {},
   "source": [
    "Entropy is always smaller than cross-entropy, meaning whenever we add new information to our model, uncertainty increases. The degree or how much bigger it is, can differ depending on the language differences, writing styles ...\n",
    "\n",
    "Note that Alligieri book has always the biggest cross-entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c64e058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order      Test Novel      Cross-Entropy  \n",
      "\n",
      "\tEntropy: 4.5090\n",
      "\n",
      "1          1               4.8576\n",
      "1          2               5.1693\n",
      "1          3               5.0138\n",
      "\n",
      "\tEntropy: 8.0591\n",
      "\n",
      "2          1               9.7857\n",
      "2          2               10.1645\n",
      "2          3               9.6282\n",
      "\n",
      "\tEntropy: 10.8014\n",
      "\n",
      "3          1               14.3029\n",
      "3          2               14.6316\n",
      "3          3               14.2151\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Order\":<10} {\"Test Novel\":<15} {\"Cross-Entropy\":<15}')\n",
    "\n",
    "for order in orders:\n",
    "    model = IIDModel(order=order)\n",
    "    model.process(novel_text)\n",
    "    print(f\"\\n\\tEntropy: {model.getEntropy():.4f}\\n\")\n",
    "    for i, test_text in enumerate(test_novels, start=1):\n",
    "        cross_entropy = model.getCrossEntropy(test_text)\n",
    "        print(f'{order:<10} {i:<15} {cross_entropy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd66ead",
   "metadata": {},
   "source": [
    "## 4. \n",
    "**Of course we were not expecting to see any meaningful sentences being generated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97b898f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: eolioeroy ieorel.alhw  ntca ita n uohi   oestb nhon  eiar nhehtih ot eoaam rsun?taeha eeewo\n",
      "ay-abrsm\n",
      "Sentence 2: srLrtn  nis  agdm i dnem  ndorfa d  \n",
      "l p\n",
      " \" ldeeeg wemmatieuroeo  e\n",
      "aw uun\"l\"  gyer ecnh,fhne  g ser\n",
      "Sentence 3: nnRRdlpeimd\n",
      "lsuiar ! !faeslveuo eh,Slhoaugarl a \n",
      "\n",
      "n ewugre ssye kiml fnad\"sveun seeh\n",
      "hioebtkehs,tkco\n",
      "Sentence 4: ib\" io  gathotvshee\n",
      "tws cdlikernesnK  sbaaoclhtfrv   ict  lboehi ntsa rfrm, aimrh wrhreh ncmo ele No\n",
      "Sentence 5:   prr levni  n-r\n",
      " aooeehh m eaiIiskly.drrtmrfah,leb r dtoa.cls oaln  henihhsehhud ?y saRee rocv' ehf\n"
     ]
    }
   ],
   "source": [
    "order = 1  \n",
    "model = IIDModel(order=order)\n",
    "model.process(novel_text)\n",
    "\n",
    "for i in range(5):\n",
    "    sentence = model.generate(100)  \n",
    "    print(f'Sentence {i+1}: {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635aed8",
   "metadata": {},
   "source": [
    "## 5.\n",
    "Here we note a big divergence between Alighieri and all the other books that lead it to be classified in its own cluster while all the other belonging to the same one.\n",
    "\n",
    "PS: We used a hirearchical clustering approach (Agglomerative clustering) that starts with each datapoint as its own cluster and bottom-up merges datapoints that are similar to each other into the same clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0af4af52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novel 1 is in cluster 1\n",
      "Novel 2 is in cluster 2\n",
      "Novel 3 is in cluster 1\n",
      "Novel 4 is in cluster 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "novels = [novel_text] + test_novels\n",
    "models = []\n",
    "for novel in novels:\n",
    "    model = IIDModel(order=1)  #Here taking order 1 \n",
    "    model.process(novel)\n",
    "    models.append(model)\n",
    "\n",
    "# Calculating the KL divergence between each pair of novels\n",
    "kl_matrix = []\n",
    "for i in range(len(models)):\n",
    "    kl_row = []\n",
    "    for j in range(len(models)):\n",
    "        kl_div = KL_divergence(models[i].probabilities, models[j].probabilities)\n",
    "        kl_row.append(kl_div)\n",
    "    kl_matrix.append(kl_row)\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=2, linkage='average')\n",
    "labels = clustering.fit_predict(kl_matrix)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f'Novel {i+1} is in cluster {label+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab4004",
   "metadata": {},
   "source": [
    "#### MARKOV CHAIN - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e80b9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class MarkovModel:\n",
    "    def __init__(self, order=1, states=None):\n",
    "        self.order = order\n",
    "        self.model = defaultdict(lambda: defaultdict(int))\n",
    "        if states:\n",
    "            self.states = sorted(states)  \n",
    "        else:\n",
    "            self.states = None\n",
    "        self.P_kl = None  # Transition matrix\n",
    "\n",
    "    def process(self, text):\n",
    "        if not self.states:\n",
    "            self.states = sorted(set(text))  # Determine states from text if not predefined\n",
    "        self.vocabulary_size = len(self.states)\n",
    "        state_index = {state: i for i, state in enumerate(self.states)}\n",
    "\n",
    "        self.P_kl = np.zeros((self.vocabulary_size, self.vocabulary_size))\n",
    "\n",
    "        for i in range(len(text) - self.order):\n",
    "            current_state = state_index.get(text[i])\n",
    "            next_state = state_index.get(text[i + self.order])\n",
    "            if current_state is not None and next_state is not None:\n",
    "                self.P_kl[next_state, current_state] += 1\n",
    "        \n",
    "        self.P_kl += 1e-6  # Add a small value to avoid zero probabilities\n",
    "        column_sums = self.P_kl.sum(axis=0)\n",
    "        self.P_kl = self.P_kl / column_sums\n",
    "\n",
    "    def compute_stationary_distribution(self):\n",
    "        pi = np.random.uniform(size=self.vocabulary_size)\n",
    "        pi /= pi.sum()\n",
    "        for _ in range(1000):\n",
    "            pi = np.dot(self.P_kl, pi)\n",
    "        return pi\n",
    "\n",
    "    def getEntropy(self):\n",
    "        pi = self.compute_stationary_distribution()\n",
    "        entropy = -np.sum(pi * np.sum(self.P_kl * np.log(self.P_kl, where=self.P_kl > 0), axis=0))\n",
    "        return entropy\n",
    "\n",
    "    def getCrossEntropy(self, other_model):\n",
    "        pi = self.compute_stationary_distribution()\n",
    "        cross_entropy = -np.sum(pi * np.sum(self.P_kl * np.log(other_model.P_kl, where=other_model.P_kl > 0), axis=0))\n",
    "        return cross_entropy\n",
    "\n",
    "    def generate(self, length=100):\n",
    "        if self.order > len(self.states):\n",
    "            raise ValueError(\"Order is greater than the number of unique states. Cannot generate sequence.\")\n",
    "        \n",
    "        # starting with a random state from the existing states\n",
    "        current_state_index = random.randint(0, self.vocabulary_size - 1)\n",
    "        generated_text = [self.states[current_state_index]]\n",
    "\n",
    "        for _ in range(1, length):\n",
    "            current_state_probs = self.P_kl[:, current_state_index]\n",
    "            if not np.any(current_state_probs > 0):\n",
    "                break  # No valid next state, we break the generation loop\n",
    "            current_state_index = np.random.choice(self.vocabulary_size, p=current_state_probs)\n",
    "            generated_text.append(self.states[current_state_index])\n",
    "\n",
    "        return ''.join(generated_text)\n",
    "    \n",
    "    def get_probabilities(self):\n",
    "        return self.P_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f4d37",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "Notable decrease in entropy from the IID model. Still a slight increase from order 1 to 2. We attribute it here to the small size of the datasets we are dealing with that cannot account for the increasing size of the state space of order 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8bc019fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order: 1, Entropy: 2.4608\n",
      "Order: 2, Entropy: 2.8201\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "\n",
    "for order in [1,2]:\n",
    "    model = MarkovModel(order=order)\n",
    "    model.process(novel_text)\n",
    "    entropy = model.getEntropy()\n",
    "    entropies.append((order, entropy))\n",
    "    print(f\"Order: {order}, Entropy: {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab93c16",
   "metadata": {},
   "source": [
    "## 2&3.\n",
    "\n",
    "Similar remarks to the IID case here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3bc1289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order      Test Novel      Cross-Entropy  \n",
      "\n",
      "\tEntropy: 2.4608\n",
      "\n",
      "1          1               6.1064\n",
      "1          2               4.3126\n",
      "1          3               4.0031\n",
      "Order      Test Novel      Cross-Entropy  \n",
      "\n",
      "\tEntropy: 2.8201\n",
      "\n",
      "2          1               5.1007\n",
      "2          2               3.9350\n",
      "2          3               3.8570\n"
     ]
    }
   ],
   "source": [
    "all_possible_characters = sorted(set(novel_text + ''.join(test_novels)))\n",
    "\n",
    "# Initializing models using the comprehensive set of states to not encounter a dimension mismatch when calculatibg KL divergence\n",
    "for order in [1, 2]:\n",
    "    base_model = MarkovModel(order=order, states=all_possible_characters)\n",
    "    base_model.process(novel_text)\n",
    "    \n",
    "    \n",
    "    print(f'{\"Order\":<10} {\"Test Novel\":<15} {\"Cross-Entropy\":<15}')\n",
    "    print(f\"\\n\\tEntropy: {base_model.getEntropy():.4f}\\n\")\n",
    "    for i, test_text in enumerate(test_novels, start=1):\n",
    "        test_model = MarkovModel(order=order, states=all_possible_characters)\n",
    "        test_model.process(test_text)\n",
    "        cross_entropy = base_model.getCrossEntropy(test_model)\n",
    "        print(f'{order:<10} {i:<15} {cross_entropy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d4e71",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "We do not see any meaningful text, Dostoevsky was chosen on purpose so we would recognize english words, but an order 2 Markov Chain model is still not complex enough to generate meaningful text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bb4f9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Fovnonjean uikorespanolt ficru\n",
      "Sentence 2: Svanore thit ce heceand ex we \n",
      "Sentence 3: Led... wexa ay hole ov Ang he.\n",
      "Sentence 4: bu bly, thonest sha wadiker be\n",
      "Sentence 5: An ranin Wandivemy edd\n",
      "s t\n",
      "\n",
      "\"G\n"
     ]
    }
   ],
   "source": [
    "order = 1\n",
    "model = MarkovModel(order=order)\n",
    "model.process(novel_text)\n",
    "\n",
    "for i in range(5):\n",
    "    sentence = model.generate(30)  \n",
    "    print(f'Sentence {i+1}: {sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346988b9",
   "metadata": {},
   "source": [
    "## 5. The clustering we had was the same as the IID case, confirming our previous claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c3c3c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_prob_dict(matrix, states):\n",
    "    prob_dict = {}\n",
    "    for i in range(len(states)):\n",
    "        for j in range(len(states)):\n",
    "            if matrix[i, j] > 0:\n",
    "                prob_dict[(states[j], states[i])] = matrix[i, j]\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3739a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novel 1 is in cluster 1\n",
      "Novel 2 is in cluster 2\n",
      "Novel 3 is in cluster 1\n",
      "Novel 4 is in cluster 1\n"
     ]
    }
   ],
   "source": [
    "novels = [novel_text] + test_novels  \n",
    "models = []\n",
    "\n",
    "for novel in novels:\n",
    "    mm = MarkovModel(order=2) \n",
    "    mm.process(novel)\n",
    "    models.append(mm)\n",
    "\n",
    "kl_matrix = []\n",
    "for i in range(len(models)):\n",
    "    kl_row = []\n",
    "    prob1 = matrix_to_prob_dict(models[i].get_probabilities(), models[i].states)\n",
    "    for j in range(len(models)):\n",
    "        prob2 = matrix_to_prob_dict(models[j].get_probabilities(), models[j].states)\n",
    "        kl_div = KL_divergence(prob1, prob2)\n",
    "        kl_row.append(kl_div)\n",
    "    kl_matrix.append(kl_row)\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=2, linkage='average')\n",
    "labels = clustering.fit_predict(kl_matrix)\n",
    "for i, label in enumerate(labels):\n",
    "    print(f'Novel {i+1} is in cluster {label+1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
